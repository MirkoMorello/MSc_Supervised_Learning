{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L11 RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks or RNNs are a family of NN for processing **sequential data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden state has a connection to itself.\n",
    "Since RNNs work on sequence, if we unfold the RNN we understand how it works during time.\n",
    "The Current Hidden state takes information from the input and the previous hidden state  \n",
    "The Main idea is exactly to process serquential data. THe hidden state h(t) can be considered as the memory of the network. It captures information about what happened in all the previous time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we don't really need to have $o^{(t)}$ at every step, because maybe we are watching a video and we don't want and don't have a label for each fram, we just want to classify at the end of the video, in the last step:\n",
    "![alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, there are CNNs that take the output at every step and it's the one that the hidden state takes back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition of RNNs\n",
    "They can be decomposed in 3 different blocks of parameters:\n",
    "- From the input to the hidden state\n",
    "- From the previous hideen state hideen state to the next hidden state\n",
    "- From the hidden state to the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental evidence is in agreement with the idea that we need enough depth in order to perform the required mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a RNNs\n",
    "\n",
    "Really similar to training a traditional NN, Backprop is used but with a twist, since the parameters are shared by all the time steps in the network, the gradient at each step depends on both:\n",
    "- The calculation of the current time step\n",
    "- The calculation of the previous time step\n",
    "\n",
    "For example: in order to calculate the gradient at the time step t04 we need to backprop gradients to the previous 3 time steps and sum all the gradients, Backpropagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem of the vanishing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply many small numbers together -> Errors due to further back time steps have smaller and smaller gradients -> Bias parameters to capture short-term dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 solutions:\n",
    "- Activation Function: ReLU prevents $f'$ to shrink gradients when $x>0$, because it returns the identity\n",
    "- Parameter initialization: Initialize weights to the identity matrix, inizialize bias by zero, this helps preventing the weights from shrinking to zero\n",
    "- Network Architecture\n",
    "\n",
    "#### Gated RNNs\n",
    "They include both Long Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU), Gated RNNs are based on the idea of creating paths through time that have derivatives that neither vanish nor explode.  \n",
    "To achieve this goal, they use connection weights that may change at every time step.\n",
    "![alt text](image-4.png)\n",
    "![alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
