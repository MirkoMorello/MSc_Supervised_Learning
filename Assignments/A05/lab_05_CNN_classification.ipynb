{"cells":[{"cell_type":"markdown","metadata":{"id":"shbM3zJAuPI9"},"source":["In this lab you will do the following steps in order:\n","\n","1. Load and normalizing the CIFAR10 training and test datasets using\n","   ``torchvision``\n","2. Define a Convolution Neural Network\n","3. Define a loss function and optimizer\n","4. Train the network on the training data\n","5. Test the network on the test data\n","\n","Using ``torchvision``, itâ€™s extremely easy to load CIFAR10.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qf9I_7NdzTf6"},"source":["How to install a different version of a package"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"S4ZhoQ3vuPI_"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms"]},{"cell_type":"markdown","metadata":{"id":"6F0vCe0kzcJ9"},"source":["Use GPU if available"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1712606114657,"user":{"displayName":"Mirko Agarla","userId":"02007240672373134675"},"user_tz":-120},"id":"5bun1lQdwoqy","outputId":"9d3e642d-6ae6-46ed-9884-4307b0b90e6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["MPS is available!  Training on GPU ...\n","cuda\n"]}],"source":["# Train on Metal GPU (M1)\n","#train_on_gpu = torch.backends.mps.is_available()\n","train_on_gpu = torch.cuda.is_available()\n","\n","if not train_on_gpu:\n","    print('CUDA is not available.  Training on CPU ...')\n","else:\n","    print('MPS is available!  Training on GPU ...')\n","    \n","device = torch.device(\"cuda\" if train_on_gpu else \"mps\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"dZyKEHctuPI_"},"source":["1. Load and normalizing the CIFAR10 training and test datasets using\n","   ``torchvision``\n","   \n","The output of [torchvision datasets](https://pytorch.org/vision/stable/datasets.html#datasets) are PILImage images of range [0, 1].\n","We [transform](https://pytorch.org/vision/stable/transforms.html) them to Tensors of normalized range [-1, 1]. Then we call the [dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n","\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"nV92dK3ruPI_"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["# Fix for SSL certificate issue with torchvision datasets download\n","# @see https://stackoverflow.com/a/49174340\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","# Normalize using mean of 0.5 and std of 0.5 to get a normalized range [-1, 1]\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","batch_size = 12500\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"]},{"cell_type":"markdown","metadata":{"id":"5nB4FDsouPJA"},"source":["Let us [show](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html#matplotlib-pyplot-imshow) some of the training images\n"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"KVCb2s_QuPJA"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train loader:\n","50000\n","{'frog': 5000, 'truck': 5000, 'deer': 5000, 'car': 5000, 'bird': 5000, 'horse': 5000, 'ship': 5000, 'cat': 5000, 'dog': 5000, 'plane': 5000}\n","torch.Size([12500, 3, 32, 32])\n","cat truck horse ship\n","\n","Test loader:\n","10000\n","{'cat': 1000, 'ship': 1000, 'plane': 1000, 'frog': 1000, 'car': 1000, 'truck': 1000, 'dog': 1000, 'horse': 1000, 'deer': 1000, 'bird': 1000}\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt  # Import library for plotting\n","import numpy as np  # Import library for numerical computations\n","from collections import Counter  # Import Counter for counting elements\n","\n","# Function to display an image\n","def imshow(image):\n","    mean=torch.tensor([0.485, 0.456, 0.406])\n","    std=torch.tensor([0.229, 0.224, 0.225])\n","\n","    # Unnormalize the image channels to [0, 1]\n","    image = image.mul(std.unsqueeze(1).unsqueeze(2))  # More efficient element-wise multiplication\n","    image = image.add(mean.unsqueeze(1).unsqueeze(2))  # Efficient element-wise addition\n","\n","    image= image.clamp(0, 1)\n","\n","    # Convert the tensor to a NumPy array\n","    npimg = image.numpy()\n","    # Plot the image using matplotlib\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # Transpose for correct display\n","\n","# ------------------ Train Loader Section ------------------\n","\n","print(\"Train loader:\")\n","\n","# Count the frequency of each class in the training set\n","stat = dict(Counter(trainset.targets))\n","\n","# Create a new dictionary with class names as keys\n","new_stat = stat.copy()\n","for k in stat.keys():\n","    new_stat[classes[k]] = stat[k]\n","    del new_stat[k]\n","\n","# Print the length of the train loader (number of batches)\n","print(len(trainset))\n","\n","# Print the class distribution in the training set\n","print(new_stat)\n","\n","# Get a batch of random training images and their labels\n","dataiter = iter(trainloader)\n","images, labels = next(dataiter)\n","\n","# Print the shape of the image tensor (batch_size, channels, height, width)\n","print(images.shape)\n","\n","# Display the images using the imshow function\n","imshow(torchvision.utils.make_grid(images))\n","\n","# Print the labels of the images\n","print(' '.join('%s' % classes[labels[j]] for j in range(4)))  # Print labels for 4 images\n","\n","# ------------------ Test Loader Section ------------------\n","\n","print(\"\\nTest loader:\")\n","\n","# Similar steps for the test loader\n","stat = dict(Counter(testset.targets))\n","new_stat = stat.copy()\n","for k in stat.keys():\n","    new_stat[classes[k]] = stat[k]\n","    del new_stat[k]\n","\n","print(len(testset))\n","print(new_stat)\n"]},{"cell_type":"markdown","metadata":{"id":"FWjfTuThuPJA"},"source":["2. Define a Convolution Neural Network.\n","[network layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"8HW6XRf7uPJB"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","net = Net()"]},{"cell_type":"markdown","metadata":{"id":"O2P0Zlbl2Stb"},"source":["Compute the receptive field of the network"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"tsX3Q7q32SgF"},"outputs":[],"source":["# This line attempts to clone a Git repository using a shell command.\n","# !git clone https://github.com/Fangyh09/pytorch-receptive-field.git\n","\n","# This line would move the downloaded directory\n","#!mv -v pytorch-receptive-field/torch_receptive_field ./\n","#!ls\n","# Import the 'receptive_field' function from the 'torch_receptive_field' library.\n","from torch_receptive_field import receptive_field\n","\n","# Calculate the receptive field of the network 'net' for an input image size of\n","# 3 channels (RGB) and 32x32 pixels. The 'receptive_field' function would analyze the network architecture and input size to determine\n","# the receptive field size for each layer and the overall network.\n","#receptive_field(net, input_size=(3, 32, 32))\n"]},{"cell_type":"markdown","metadata":{"id":"hmA4DkACuPJB"},"source":["3. Define a loss function and optimizer\n","\n","Let's use a Classification [Cross-Entropy](https://pytorch.org/docs/stable/nn.html#loss-functions) loss and [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) with momentum.\n","\n"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"yl7S3NpruPJB"},"outputs":[],"source":["import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"]},{"cell_type":"markdown","metadata":{"id":"bmu1-dvfuPJB"},"source":["4. Train the network on the training data\n","\n","\n","We simply have to loop over our data iterator, and feed the inputs to the\n","network and optimize.\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"928O4nWYuPJC"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,     4] loss: 2.306\n","[2,     4] loss: 2.306\n","[3,     4] loss: 2.306\n","[4,     4] loss: 2.306\n","[5,     4] loss: 2.305\n","[6,     4] loss: 2.305\n","[7,     4] loss: 2.305\n","[8,     4] loss: 2.305\n","[9,     4] loss: 2.305\n","[10,     4] loss: 2.305\n","[11,     4] loss: 2.305\n","[12,     4] loss: 2.305\n","[13,     4] loss: 2.305\n","[14,     4] loss: 2.305\n","[15,     4] loss: 2.305\n","[16,     4] loss: 2.305\n","[17,     4] loss: 2.305\n","[18,     4] loss: 2.305\n","[19,     4] loss: 2.305\n","[20,     4] loss: 2.305\n","[21,     4] loss: 2.305\n","[22,     4] loss: 2.304\n","[23,     4] loss: 2.304\n","[24,     4] loss: 2.304\n","[25,     4] loss: 2.304\n","[26,     4] loss: 2.304\n","[27,     4] loss: 2.304\n","[28,     4] loss: 2.304\n","[29,     4] loss: 2.304\n","[30,     4] loss: 2.304\n","[31,     4] loss: 2.304\n","[32,     4] loss: 2.304\n","[33,     4] loss: 2.304\n","[34,     4] loss: 2.304\n","[35,     4] loss: 2.304\n","[36,     4] loss: 2.304\n","[37,     4] loss: 2.304\n","[38,     4] loss: 2.304\n","[39,     4] loss: 2.303\n","[40,     4] loss: 2.303\n","[41,     4] loss: 2.303\n","[42,     4] loss: 2.303\n","[43,     4] loss: 2.303\n","[44,     4] loss: 2.303\n","[45,     4] loss: 2.303\n","[46,     4] loss: 2.303\n","[47,     4] loss: 2.303\n","[48,     4] loss: 2.303\n","[49,     4] loss: 2.303\n","[50,     4] loss: 2.303\n","Finished Training\n"]}],"source":["net.to(device)\n","\n","for epoch in range(50):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0  \n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        # move to GPU\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        \n","    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / len(trainloader):.3f}')\n","    running_loss = 0.0\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"bzpMI9aUuPJC"},"source":["5. Test the network on the test data\n","\n","\n","We have trained the network for 2 passes over the training dataset.\n","But we need to check if the network has learnt anything at all.\n","\n","We will check this by predicting the class label that the neural network\n","outputs, and checking it against the ground-truth. If the prediction is\n","correct, we add the sample to the list of correct predictions.\n","\n","\n"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"e_JnNda2uPJD"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the network on the 10000 test images: 8 %\n"]}],"source":["# Initialize variables to track accuracy\n","correct = 0\n","total = 0\n","\n","# Disable gradient calculation for better performance during evaluation\n","with torch.no_grad():\n","    # Loop over the test loader\n","    for data in testloader:\n","        # Get the image and label from the current batch\n","        image, label = data\n","\n","        # Move the image data to the specified device (CPU or GPU)\n","        image = image.to(device)\n","\n","        # Get the network's prediction for the image\n","        output = net(image)\n","        # smax = torch.nn.Softmax(dim=1)(output.cpu())\n","\n","        # Find the class with the highest probability\n","        _, predicted = torch.max(output.cpu(), 1)  # Equivalent to pred = torch.argmax(output.cpu(), dim=1)\n","\n","        # Update total number of test images\n","        total += label.size(0)  # label.size(0) gives the batch size\n","\n","        # Count correct predictions\n","        correct += (predicted == label).sum().item()  # Count true positives\n","\n","# Calculate and print accuracy\n","print('Accuracy of the network on the 10000 test images: %d %%' % (\n","    100 * correct / total))\n"]},{"cell_type":"markdown","metadata":{"id":"O1Y4HMBOxCE6"},"source":["**!HOMEWORK!**"]},{"cell_type":"markdown","metadata":{"id":"mZW_s0fsB1Xi"},"source":["This homework assignment asks you to performs 2 tasks:\n","\n","1. Analyze Results with Different Network Parameters:\n","\n","This involves training the network with various configurations of network parameters and analyzing the impact on performance. Here's a step-by-step approach:\n","\n","**Choose Network Parameters:**\n","\n","Select the network parameters you want to experiment with. Common choices include:\n","\n","Number of layers: You can try increasing or decreasing the number of layers in your chosen network architecture (e.g., convolutional layers in a CNN).\n","Learning rate: Experiment with different learning rates (e.g., 0.01, 0.001, 0.0001) to find a balance between fast learning and stability.\n","Other parameters: Depending on your network architecture, there might be additional options like:\n","Number of filters in convolutional layers: This affects the complexity of features extracted from the data.\n","Activation functions: Experiment with different activation functions (e.g., ReLU, Leaky ReLU) to introduce non-linearity.\n","Optimizer parameters: Some optimizers (e.g., Adam) have hyperparameters you can adjust.\n","Train the network for a different number of epochs.\n","\n","**Analyze Results:**\n","\n","Compare the performance of the network across different parameter configurations:\n","\n","How accuracy/loss changes with different parameter values.\n","2. Show and Explain Errors of the Best Network:\n","\n","Once you identify the **best performing network configuration** (based on metrics like accuracy or loss), analyze its errors.\n","For example you can generate a confusion matrix. This matrix visualizes how often the network predicted each class correctly or incorrectly.\n","\n","Useful resources:\n","*   [network layers](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n","*   [activation function](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n","*   [loss functions](https://pytorch.org/docs/stable/nn.html#convolution-layers)\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
